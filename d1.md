1.  配置集群
a. software
    zsh, oh-my-zsh, vim, vimrc, 
    python(anaconda3), tensorflow, tf_py27
    torch
    linklink, pytorch

b. data
    cityscapes

2.  集群使用说明
a. 登陆的是管理节点，不能占用太多资源，因为大家都登录同一个管理节点
b. 使用sinfo查看分区信息，使用squeue查看队列信息
c. srun --mpi=pmi2 -p $1 (分区名称) -n32（想要申请的显卡数目） --gres=gpu:8 （每个节点8个显卡） --ntasks-per-node=8（任务数对应于显卡数） --cpus-per-task=5（cpu线程个数） \
d. --n16 总任务数（16*batchsize=总共的batchsize）；Slurm会根据这个数和-ntasks-per-node，每个节点多少个任务（8个卡）
   --gres=gpu:4 单个节点使用的gpu个数；一般这个数和-ntasks-per-node相等。


3.  代码到底在哪？-管理节点就可以？
    环境在哪？-管理节点？
    所以srun只分配资源？

[TODO]
1. 下载Dataset，--服务器多卡跑torch
                --推到集群上试试
2. 管理节点，cudnnlib位置改变，变为~/software/cuda/lib64后，my_dwt可以本地，也可以推到集群上运行。
            但是，推到集群上后显示了4次分配4个显卡，这时用的参数是-n4, gres=4, ntasks=4
3. 看imagenet_example 代码
    a. 如何读的batch，只有4张图片，batchsize为1，为什么只有一个卡工作，其他三个卡是out of memory
    b. 什么时候用的mpii，是不是只有node大于2时才会使用？
    c. 看流程，学pytorch

